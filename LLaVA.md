# LLaVA

**论文**：*Visual Instruction Tuning*

## 1. 问题

### 1.1. 概述

开发语言增强的基础视觉模型在开放世界的视觉理解方面表现出色，例如分类、检测、分割和描述，以及视觉生成和编辑。但每个任务都由一个大型视觉模型独立解决，任务的具体要求在模型设计阶段就已经被固定下来，语言仅用于描述图像内容，用户并不能通过语言对任务类型进行指定和灵活切换。

LLM 已经表明语言可以发挥更广泛的作用，作为通用助手的通用接口，各种任务指令都可以用语言明确表示，并引导进行了端到端训练的模型切换到用户通过语言指定的任务。

而要在视觉语言模型上实现类似效果，必须首先解决多模态指令遵循数据缺乏的问题，以获取足够的数据用于模型的训练。

### 1.2. 重要性

人工智能的一个核心目标是开发出一种通用助手，能够有效地遵循多模态的视觉和语言指令，与人类意图保持一致，从而在各种现实场景中完成各种任务。但目前的视觉模型只能完成在模型设计阶段就固定的任务。

为实现该核心目标，必须获取足够的多模态指令遵循数据，通过某种方法训练视觉语言模型根据语言切换自身任务类型，构建能够遵循多模态指令的视觉语言模型。

## 2. 回顾

在该论文之前人们进行的相关研究主要是多模态指令遵循代理和指令微调。

* **多模态指令遵循代理** CV 领域此前构建指令遵循代理的工作大致分为两种：针对每个独立的、特定的任务分别进行探索，进行端到端训练，实现能完成固定单一任务的视觉语言模型；搭建通过 LangChain / LLMs 来协调各种模型的系统，如 Visual ChatGPT、X-GPT、MM-REACT、VisProg、ViperGPT。

* **指令微调** 在 NLP 领域，为了让 LLM 能够遵循自然语言指令并完成现实世界中的任务，人们针对这些模型探索了对其进行指令微调的方法，产生了诸如 InstructGPT / ChatGPT、FLAN-T5、FLAN-PaLM、OPT-IML 等指令微调后的模型。事实证明，指令微调有效提升了大型语言模型的零样本和少样本泛化能力。

  为在 CV 领域引入指令微调的理念，人们训练了许多基于图像文本对训练的 LMM，包括Flamingo、BLIP-2、FROMAGe、KOSMOS-1。并在 LLaMA 的基础上实现了 OpenFlamingo 和 LLaMA-Adapter，使得 LLaMA 能够处理图像输入。

  虽然这些模型在任务迁移泛化性能方面表现出色，但由于未专门使用视觉语言指令数据进行调优，在多模态任务中他们的表现通常不如纯语言任务，而视觉指令调优可以有效解决该问题。

## 3. 方法

### 3.1. 视觉指令生成

考虑到图像对数据的巨大规模，并受 GPT 模型在文本标注任务中取得成功的启发，作者提出利用 ChatGPT/GPT-4 进 行多模态指令遵循数据收集。

为生成多模态指令遵循数据，作者将由一张图片及其说明文字组成的 *图像-文字对* 输入 GPT-4，令其据此生成一组指令，这组指令会引导视觉语言模型依据图像信息完成不同的任务；需要注意的是 GPT-4 并不能获取输入的图像信息，而是只根据文字信息生成指令。

但这样生成的指令以及根据指令做出的回应都相对单一，且缺乏多样性、不涉及复杂推理，难以应对更复杂的指令遵循场景。

为解决该问题，使用两种类型的符号表示，将图像编码为视觉特征以提示纯文本 GPT：

* **图像说明** 从不同角度对图像的整体内容和细节进行描述。

* **边界框** 标注图像中各个对象的位置，每个边界框包含对象的类别和空间位置信息。

借此，图像被编码为 LLM 可识别的序列，然后使用图像数据集（在该论文中为 COCO）生成三种类型的指令遵循数据，分别是对话、详细描述、复杂推理。对于每种类型，首先手动设计几个种子实例，即最基础的依据 *图像-文字对* 转化得到的 *指令-响应对*； LLM 会按照种子实例提供的标准格式，以这几个实例为模板通过增添细节、引入上下文、修改指令内容等方式生成后续的指令遵循数据。

### 3.2. 视觉指令调优

对于一组 *图像-文字对*，将图像信息输入预训练的 CLIP 视觉编码器 ViT-L/14，同时考虑模型中 Transformer 结构最后一层之前以及之后的图像特征输出，通过一个可训练的投影层将其转化为语言嵌入标记；采用全连接层进行转化具有实现简单、计算开销小的优点，有助于提升实验与迭代的效率，但这不一定是最优方法，对更复杂方案的探索也在未来的研究计划中。

接下来将语言嵌入标记、*图像-文字对* 中的文本信息、*指令-响应对* 中的指令部分拼接为一个整体序列作为输入，*指令-响应对* 中的响应部分作为目标输出，对 Vicuna 进行训练。需要注意的是一个 *图像-文字对* 会与多个针对不同任务的 *指令-响应对* 分别拼接为多个整体序列，借此训练模型根据不同指令对同一 *图像-文字对* 执行不同的任务。

在指令微调时采用 LLM 常用的自回归训练目标，根据输入和上文预测下文的概率分布，使目标答案对应概率最大。指令微调具体分为以下两个阶段：

* **特征对齐预训练** 冻结视觉编码器和语言编码器，将 CC3M 过滤至 595K 个 *图像-文本对*，并据此生成指令遵循数据，通过训练连接视觉编码器和语言编码器的投影层，实现图像特征与预训练 LLM 词嵌入的对齐。

* **端到端微调** 冻结视觉编码器，在以下两种具体的使用场景上对投影层和语言编码器进行联合训练：
  
  * *多模态聊天机器人* 通过 158K 条语言-图像指令跟随数据进行微调，均匀采样三种类型的响应，其中对话是多轮的，详细描述和复杂推理是单轮的。

  * *科学问答* 在 ScienceQA 基准测试集上进行微调，每个问题都以自然语言或图像的形式提供上下文。模型接受问题和上下文作为输入，以推理过程和答案作为目标输出，用自然语言提供推理过程，并从多个选项中选择答案。

## 4. 实验

主要包含两种实验设置：多模态聊天机器人和 ScienceQA 数据集。使用 8 块 A100 显卡对所有模型进行训练，遵循 Vicuna 的超参数设置。先在过滤后的 CC-595K 子集上对模型进行 1 个周期的预训练，lr = 2e-3，batch = 128，然后在该论文提出的 LLaVA-Instruct-158K 数据集上进行 3 个周期的微调，lr = 2e-5，batch = 32。

* **多模态聊天机器人** 作者开发了一个聊天机器人演示程序，以展示 LLaVA 的图像理解和对话能力。在基于一个规模较小的多模态指令跟随数据集（约 8 万张独特图像）进行训练后，该聊天机器人所展示的推理结果与多模态 GPT-4 接近，并能够理解领域外数据的场景，根据问题指令给出合理的回答。相比之下，BLIP-2 和 OpenFlamingo 更侧重于描述图像，而非根据用户指令以恰当的方式进行回答。

  对于模型性能，采用以下两个基准进行评估：

  * *LLaVA-Bench(COCO)* 从 COCO-Val-2014 中随机选取 30 张图片，针对每张图片，生成对话、详细描述、复杂推理三种类型的问题总计 90 个。通过改变训练数据集，研究不同类型指令跟随数据的有效性。结果显示，模型遵循用户指令的能力在使用某一类数据进行指令微调后提升了 50% 以上。模型的整体能力在添加少量的详细描述和复杂推理问题后提升了 7%，推理能力的提升还提升了模型在对话问题上的表现。而在同时使用三种数据的情况下，模型准确率达到 85.1%。

  * *LLaVA-Bench(真实场景)* 使用 24 张多样化的图片，涵盖室内和室外场景、表情包、绘画、 素描等，并为每张图片关联高度详细的、人工整理的描述以及精心挑选的问题，作为数据集对 LLaVA、BLIP-2、OpenFlamingo 进行训练；借此评估模型在挑战性任务中的能力以及对新领域的泛化能力。结果显示，LLaVA 相比 BLIP-2 提升了 29%，相比 OpenFlamingo 提升了 48%，相比能够访问真实标签的纯文本 GPT-4，LLaVA 可以达到其表现的 67.3%，仅对复杂推理问题可以达到 81.7%。

  但 LLaVA 也有其限制，图像被其视作特征的集合，难以捕捉特征之间更复杂的相互关系，对复杂语义的理解能力有限。

* **ScienceQA** ScienceQA 包含 21000 多道多模态多项选择题，涵盖 3 个学科、26 个主题、127 个类别和 379 项技能，分为训练集、验证集和测试集，分别包含 12726 个、4241 个 和 4241 个示例。将通过投影层之前的视觉特征作为 LLaVA 的输入，让模型先输出推理过程，再输出答案，并训练 12 个周期，正确率达到了 90.92%（该论文此前最佳为 91.68%）。

  为探究大语言模型的极限，使用 2-shot 上下文学习对 GPT-4 进行提示，提供两组 *问题-推理-答案* 和一个新问题作为输入，让模型现场借助上文学习解决当前任务的方法，准确率达到 82.69%。

  然后使用两种方法结合 LLaVA 和 GPT-4：

  * *GPT-4 补充* 当 GPT-4 无法回答，使用 LLaVA 的回答。准确率为 90.97%。

  * *GPT-4 裁判* 当 GPT-4 和 LLaVA 产生不同答案时，让 GPT-4 根据问题和两个结果再次回答。该方案能够在所有问题类别中持续提升表现，并达到了 92.53% 的新最佳准确率。值得注意的是，纯文本 GPT 提高了 LLaVA 在图像背景问题上的整体性能，作者推测这是因为 GPT-4 能够纠正 LLaVA 在一些实际上无需视觉信息的问题上的错误。

  作者还进行了一系列消融实验：
  
  * 换用 CLIP 的倒数第二层特征，准确率下降了 0.96%。
  
  * 不输出推理过程，收敛速度更慢，但准确率无显著差异。
  
  * 跳过预训练，准确率下降了 5.11%。
  
  * 7B 规模的模型相比 13B，准确率下降了 1.08%。

## 5. 点评

### 5.1. 优势

* **高效数据生成** 借助 GPT-4 系统性地生成多模态指令遵循数据，解决了多模态大模型训练指令遵循数据缺乏的问题。

* **实验全面** 设计了大量实验，从不同方面，以不同任务，对 LLaVA 的性能进行系统评估，并针对实验结果对可能的原因进行推测。

* **通用性强** 对于同一个 *图像-文本对*，不同的指令可以训练模型对其执行不同的任务，并根据指令在不同的任务间自行切换。

* **开源资源** 开源了研究过程中的全部数据集、源代码、模型权重、可视化演示等，促进实验复现、方法改进、相关研究与应用工作的展开。

### 5.2. 不足

* **复杂语义理解能力差** LLaVA 将图像视作特征集合，缺乏对复杂场景和深层语义的理解。

* **人工设计数据** GPT 生成数据依赖人工生成的高质量种子实例。

* **视觉特征局限** 图像编码器保持冻结，无法与投影层进行任何交互，可能对模型提取视觉特征的效果有一定限制。
