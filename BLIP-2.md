# BLIP-2:

**论文**：*Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models*

## 1. 问题

### 1.1. 概述

VLP 研究中大多数最先进的视觉语言模型在预训练期间，由于使用大规模模型和数据集进行端到端训练，计算成本高昂，且视觉语言模型还不能从视觉和自然语言领域中现成的单模态模型中获益。因此作者希望研究一种预训练策略以解决这一问题，促进模型的跨模态对齐，利用预训练的且处于冻结状态的单模态模型（降低计算成本并解决灾难性遗忘问题）进行视觉语言处理。

### 1.2. 重要性

过去的 VLP 研究都选择对大规模数据集和模型进行端到端训练，对于现存的许多已完成预训练的单模态模型来说，重新进行端到端训练是对计算资源的一种严重浪费，对训练效果也会带来严重的负面影响。一种能够在单模态模型冻结的情况下实现跨模态对齐的方法可以有效解决上述问题；BLIP-2 正是这样一种通用且高效的预训练策略，不仅可训练参数比现有方法少得多，在各种视觉语言任务中还均达到了最先进的性能，既节约了计算资源又提升了模型表现。

## 2. 回顾

在 BLIP-2 被提出之前，人们已经提出了多种模型架构和预训练目标。主流的视觉语言模型都采用端到端的训练方式，需要消耗大量数据和计算资源，并且很少直接利用已经完成预训练的单模态模型。

也有一些人尝试利用现成的预训练模型，并在视觉语言预训练过程中保持这些模型不变。其中既有冻结图像编码器的方法也有冻结语言模型的方法，这两种方法都采用语言建模损失，即模型根据图像生成文本，并与真实文本比较计算损失。但这些尝试也只冻结两个单模态模型的其中之一，对另一个采用端到端训练方式，消耗的计算资源也很大，且仍未能彻底利用预训练单模态模型。

## 3. 方法

BLIP-2 开创性地采用了双冻结的方法，即同时冻结图像和语言两个单模态预训练模型，利用一个被称为 Q-former 的结构完成多模态对齐，在整个预训练过程中只需要对 Q-former 的参数进行优化，需要训练的参数量显著减少，并且模型的表现在多项视觉语言任务中都超过了此前采用端到端方法进行预训练的大模型的表现。

作者将图像输入 ViT 等图像编码器得到图像特征，并将配对的文本信息输入 BERT 等文本编码器得到特征向量 t。

对于 Q-former，首先初始化 32 个 768 维的查询作为一组查询嵌入，这组可学习的查询嵌入被 Q-former 接受作为输入，通过自注意力层整合彼此信息，若该 Transformer 块插入了交叉注意力层，还会将图像特征与查询输入共同通过交叉注意力层，查询输入会在这一步中接受视觉信息，并且每个查询输入会关注图像特征的不同方面；原作者选择每隔一层插入一次交叉注意力层，即每通过自注意力层两次会通过交叉注意力层一次。然后分别采取 ITC、ITG、ITM 三种任务目标进行学习。

* **ITC** 每个查询输出分别与 t 计算相似度，相似度的最大值称为查询分数。目标为尽可能增大一个 batch 中配对的图像与文字的查询分数，降低不配对图像与文字的查询分数来优化 Q-former 的参数。查询分数选取相似度最大值的做法使得每次只关注一个查询向量和 t 之间的匹配程度，优化只调整其中一个特征向量的参数，虽然略微降低了优化效率，但提升了每次优化的效果。

* **ITG** 整组查询输出作为 LLM 的输入，根据不同结构的 LLM，ITM 采取不同的优化方式：
*基于解码器的 LLM* LLM 会根据视觉特征和上文预测下一个 token，目标为减少与真实 token 之间的损失。
*基于编码器-解码器的 LLM* 将文本信息分为前缀和后缀两部分，前缀与图像特征一同输入 LLM 编码器，目标为减少输出与后缀之间的损失。

* **ITM** 整组查询输出与文本信息拼接在一起，输入双向自注意力掩码的 Transformer 层以整合二者信息，然后输入一个用于二分类任务的模型中，输出一个概率表示对 *匹配/不匹配* 进行的分类，目标为尽可能增大真实配对的图像和文本输出 *匹配* 的概率，随机配对的输出 *不匹配* 的概率。

分别对两个单模态大模型进行预训练，图像编码器选取 ViT-L/14 和 ViT-g/14，语言编码器选取基于 encoder 的 OPT 和基于 encoder-decoder 的 FlanT5。在选取合适的参数和数据集完成单模态大模型预训练后将其冻结，并进行 Q-former 的训练以实现多模态对齐。

## 4. 实验

作者在多个任务上对基于 BLIP-2 方法进行训练的大模型上进行评估，并与其他多模态大模型进行比较以验证效果。较为具体的操作过程如下：

* **零成本视觉问答** 在 0 样本视觉回答任务上进行定量评估，使用宽度为 5 的束进行搜索，并将长度惩罚设置为 -1，鼓励更简短的回答，使其更符合人工标注。将 BLIP-2 与 Flamingo80B 在  VQAv2 和 GQA 两个数据集上的表现进行对比，结果显示 BLIP-2 在可训练参数减少 54 倍的情况下表现提升了 8.7%，且为 BLIP-2 换用更强的单模态大模型可以提升其表现；这证实了 BLIP-2 是一种通用的视觉语言预训练方法，能高效利用单模态大模型的成果。

  该实验还证实了 ITG 和 ITM 这两种视觉语言表征学习的效果。在缺少视觉语言表征学习的情况下，Q-former 仅依靠视觉到语言的生成式学习来弥合模态差异，这与 Flamingo 中的 Perceiver Resampler 类似，会导致两种类型的大型语言模型在零样本视觉问答任务上的表现大幅下降，尤其是在使用存在灾难性遗忘问题的 OPT 作为语言编码器时，其性能会随着训练的进行急剧下降。

* **图像描述** 冻结语言编码器，对 ViT-g 和各种语言模型进行实验，在 COCO 数据集上进行微调，并在 COCO 测试集和零样本迁移至 NoCaps 验证集上进行评估。结果显示 BLIP-2 达到了当前最优水平。

* **视觉问答** 冻结语言编码器，在该人物中，特殊的是 LLM 的输入不仅包含 Q-former 的输出，还包含原始的问题 token 序列，以将问题本身也作为 LLM 的一部分条件。所用的数据集来自 VQAv2 的训练集和验证集，以及 Visual Genome 的训练样本。结果显示 BLIP-2 在不同数据集，不同任务，不同召回率指标下表现均优异，且多次超过作为对照的其他模型达到最佳水平。

* **图像-文本检索** 由于该任务不涉及语言生成，直接在没有 LLM 的情况下对预训练模型进行微调。具体来说，在 COCO 数据集上对图像编码器和 Q-Former 进行微调，使用与预训练相同的任务目标，在 COCO 和 Flickr30K 数据集上对模型进行 *图像到文本检索* 和 *文本到图像检索* 的评估。在推理过程中，首先基于图像-文本特征相似度选择 k = 128 个候选对象，然后基于成对的 ITM 得分进行重新排序。分别使用 ViT-L 和 ViT-g 作为图像编码器。结果显示 BLIP-2 取得了最先进的性能，显著优于现有方法。

  由于 ITC 和 ITM 直接学习图像-文本特征相似度，对于该任务而言尤其重要，但实验结果也显示 ITG 在该任务中同样提升了模型表现。这证实了 ITG 促使查询嵌入提取与文本最相关的视觉特征，从而促进了多模态对齐。

## 5. 点评

### 5.1. 优势

* **创新性强** 首次提出了将单模态大模型都冻结的多模态大模型预训练方法，大大减少了多模态大模型的预训练耗时与成本，甚至提升了预训练效果。

* **高效利用预训练单模态大模型** BLIP-2 方法在单模态大模型冻结的情况下进行预训练，无需进行端到端训练，模型在预训练开始前就已经获取了预训练单模态大模型已掌握的知识，无需再次学习。

* **通用性强** BLIP-2 方法只对 Q-former 进行训练，图像编码器和语言编码器可以根据实际需要选取不同的预训练单模态大模型，使得该预训练方法可以广泛应用于不同的多模态大模型的预训练中。

* **实验完善** 在多种不同的视觉语言方面的任务中对 BLIP-2 均进行了评估，充分证明了 BLIP-2 良好的训练效果和广泛的适用场景。

* **节约计算资源** 由于 Q-former 的参数量相对于端到端训练很少，此后的多模态大模型可以用相比之前很短的时间和很少的计算资源实现相同效果甚至更好的预训练，也降低了模型迁移到新任务所需的成本。

### 5.2. 不足

* **依赖单模态大模型的预训练成果** BLIP-2 的性能依赖于选取的单模态大模型的能力，难以在预训练中针对不同的任务场景对单模态大模型也做出一定的优化。

* **模态对齐局限** 多模态对齐完全依赖 Q-former 的训练，图像编码器和语言编码器无法进行任何交互，可能对多模态对齐的效果有一定限制。
