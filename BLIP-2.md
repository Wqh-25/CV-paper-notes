# BLIP-2:

**论文**：*Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models*

## 1. 问题

### 1.1. 概述

VLP 研究中大多数最先进的视觉语言模型在预训练期间，由于使用大规模模型和数据集进行端到端训练，计算成本高昂，且视觉语言模型还不能从视觉和自然语言领域中现成的单模态模型中获益。因此作者希望研究一种预训练策略以解决这一问题，促进模型的跨模态对齐，利用预训练的且处于冻结状态的单模态模型（降低计算成本并解决灾难性遗忘问题）进行视觉语言处理。

### 1.2. 重要性

过去的 VLP 研究都选择对大规模数据集和模型进行端到端训练，对于现存的许多已完成预训练的单模态模型来说，重新进行端到端训练是对计算资源的一种严重浪费，对训练效果也会带来严重的负面影响。一种能够在单模态模型冻结的情况下实现跨模态对齐的方法可以有效解决上述问题；BLIP-2 正是这样一种通用且高效的预训练策略，不仅可训练参数比现有方法少得多，在各种视觉语言任务中还均达到了最先进的性能，既节约了计算资源又提升了模型表现。

## 2. 回顾

在 BLIP-2 被提出之前，人们已经提出了多种模型架构和预训练目标。主流的视觉语言模型都采用端到端的训练方式，需要消耗大量数据和计算资源，并且很少直接利用已经完成预训练的单模态模型。

也有一些人尝试利用现成的预训练模型，并在视觉语言预训练过程中保持这些模型不变。其中既有冻结图像编码器的方法也有冻结语言模型的方法，这两种方法都采用语言建模损失，即模型根据图像生成文本，并与真实文本比较计算损失。但这些尝试也只冻结两个单模态模型的其中之一，对另一个采用端到端训练方式，消耗的计算资源也很大，且仍未能彻底利用预训练单模态模型。

## 3. 方法

BLIP-2 开创性地采用了双冻结的方法，即同时冻结图像和语言两个单模态预训练模型，利用一个被称为 Q-former 的结构完成多模态对齐，在整个预训练过程中只需要对 Q-former 的参数进行优化，需要训练的参数量显著减少，并且模型的表现在多项视觉语言任务中都超过了此前采用端到端方法进行预训练的大模型的表现。

作者将图像输入 ViT 等图像编码器得到图像特征，并将配对的文本信息输入 BERT 等文本编码器得到特征向量 t。

对于 Q-former，首先初始化 32 个 768 维的查询作为一组查询嵌入，这组可学习的查询嵌入被 Q-former 接受作为输入，通过自注意力层整合彼此信息，若该 Transformer 块插入了交叉注意力层，还会将图像特征与查询输入共同通过交叉注意力层，查询输入会在这一步中接受视觉信息，并且每个查询输入会关注图像特征的不同方面；原作者选择每隔一层插入一次交叉注意力层，即每通过自注意力层两次会通过交叉注意力层一次。然后分别采取 ITC、ITG、ITM 三种方法进行学习。

* **ITC** 每个查询输出分别与 t 计算相似度，相似度的最大值称为查询分数。通过尽可能增大一个 batch 中配对的图像与文字的查询分数，降低不配对图像与文字的查询分数来优化 Q-former 的参数。查询分数选取相似度最大值的做法使得每次只关注一个查询向量和 t 之间的匹配程度，优化只调整其中一个特征向量的参数，虽然略微降低了优化效率，但提升了每次优化的效果。

* **ITG** 整组查询输出作为 LLM 的输入，根据不同结构的 LLM，ITM 采取不同的优化方式：
*基于解码器的 LLM* LLM 会根据视觉特征和上文预测下一个 token，计算与真实 token 之间的损失对 Q-former 进行优化。
*基于编码器-解码器的 LLM* 将文本信息分为前缀和后缀两部分，前缀与图像特征一同输入 LLM 编码器，计算输出与后缀之间的损失对 Q-former 进行优化。

* **ITM** 整组查询输出与文本信息拼接在一起，输入双向自注意力掩码的 Transformer 层以整合二者信息，然后输入一个用于二分类任务的模型中，输出一个概率表示对 *匹配/不匹配* 进行的分类，通过尽可能增大真实配对的图像和文本输出 *匹配* 的概率，随机配对的输出 *不匹配* 的概率对 Q-former 进行优化。
