# YOLO

**论文**：*You Only Look Once: Unified, Real-Time Object Detection*

## 1. 问题

### 1.1. 概述

目标检测是 CV 领域的核心任务之一，该论文之前的目标检测方法具有运行速度慢、难以优化等等多种缺点，与人类在处理视觉信息时的表现还有着极大的差距。该论文旨在提出一种全新的目标检测方法 YOLO，使视觉模型能够像人类一样，在极短的时间内精准掌握图像中存在的物体、物体的位置、彼此之间的相互作用等信息。

### 1.2. 重要性

快速且精准的物体检测算法将极大程度地推动目标检测乃至整个 CV 领域的发展，使计算机无需专用传感器就能驾驶汽车，让辅助设备能够实时向用户传达场景信息，并为通用型、响应迅速的机器人系统的开发打开大门。

## 2. 回顾

此前的检测系统会采用特定对象的分类器，在测试图像的不同位置和尺度上检测该对象。如 DPM 采用滑动窗口法，在图像上以均匀间隔的位置运行分类器。

更先进的方法，如 R-CNN，使用区域提议法进行目标检测。具体来说，首先在图像中生成潜在的边界框，对边界框分别运行分类器，然后优化边界框的位置和尺度，对重叠度高的边界框仅保留置信度最高的一个，最后根据图像中其他对象的分布和关系，重新调整每个边界框的置信度。

以上传统目标检测方法由于流程过于复杂，且有大量需要单独优化的组件，因此有着设计与实现复杂、运行速度慢、优化难度大、资源消耗大、的显著缺点。

## 3. 方法

### 3.1. 算法

YOLO 首先将输入图像划分为一个 S * S 的网格，每个网格单元预测 B 个边界框及其置信度。置信度定义为 Pr(Object) * IOU(truth-pred)，Pr(Object) 表示该网格单元内存在任意对象的概率，IOU(truth-pred) 表示该边界框与真实物体的重叠程度。在每个网格单元内 Pr(Object) 是相同的，置信度正比于该边界框的 IOU(truth-pred)。若网格单元内不存在任何对象，边界框的置信度的目标应为 0，否则应等于其 IOU(truth-pred)。

### 3.2. 输出

在 YOLO 的输出中，边界框对应五个预测值：x, y, w, h 和置信度。(x, y) 表示边界框中心相对于网格单元中心的位置，(w, h) 是边界框宽度和高度相对于网格单元宽度和高度归一化后的值；网格单元对应的预测值包含其中每个边界框对应的预测值，以及 C 个条件类别概率，分别表示该网格单元包含检测的 C 类对象中某一种的概率。

在进行测试时，网格单元的条件类别概率与其中边界框的置信度分别相乘，得到每个边界框的特定了别置信度，表示一个边界框包含某一类对象的概率，以及该边界框与与该对象的重叠程度。

综上所述，YOLO 的最终输出应当是一个尺寸为 S * S * (B * 5 + C) 的张量。举例来说，在 PASCAL VOC 数据集上评估 YOLO 时，S = 7, B = 2, C = 20，因此最终输出是一个 7 * 7 * 30 的张量。

### 3.3. 设计

YOLO 的架构受到 GoogleNet 启发，但不同于 GoogleNet 采用结构复杂的 inception 块，YOLO 由 24 个卷积层和 2 个全连接层组成。卷积层会首先用 1 * 1 卷积核减少通道数，再用 3 * 3 卷积核提取特征。Fast YOLO 则只包含 9 个卷积层，且卷积核数量更少，以使运行速度更快。

### 3.4. 训练

在 ImageNet 1000-class competition dataset 上对卷积层进行预训练，进行图像分类任务。使用 20 个卷积层，连接一个平均池化层和一个全连接层。使用 Darknet 框架进行训练和推理，在约一周的训练后，该网络在 ImageNet 2012 验证集上的单裁剪 top-5 准确率为 88%，与 Caffe 模型库中的 GoogleNet(Inception Net) 模型相当。

然后迁移到检测任务，移除平均池化层和全连接层，连接四个卷积层和两个全连接层，权重均进行随机初始化，输入分辨率从 224 * 224 提高至 448 * 448。对模型的最后一层使用线性激活函数，其他层使用 Leaky ReLU 激活函数( f(x) = x if x > 0 else 0.1x )。

对于损失函数，模型使用易于优化的平方和损失。但平方和损失同等对待定位损失和分类损失，且由于许多网格单元实际上并不含对象，这些网格中边界框的置信度损失过大，定位损失和分类损失对总损失的影响较小，可能导致模型训练不稳定甚至发散的问题，不符合最大化平均精度的目标。因此引入 λcoord 和 λnoobj 两个参数，λcoord 用于放大包含物体的边界框的定位损失，设为 5，λnoobj 用于缩小不包含物体的边界框的置信度损失，设为 0.5。此外，平方和损失无法体现边界框大小的差异，因此在计算 w 和 h 的损失时改用 $\sqrt{w}$ 和 $\sqrt{h}$，使得小框对 w 和 h 的误差更敏感。

在损失函数中引入参数 $\mathbb{1}_{ij}^{obj}$，每次训练时，对于某个对象，其所属网格单元 $\mathbb{1}_{i}^{obj} = 1$，其他网格单元 $\mathbb{1}_{i}^{obj} = 0$。计算其所属网格单元内每个边界框与其的 IOU，认为 IOU 最大的边界框负责该对象的识别，该边界框 $\mathbb{1}_{ij}^{obj} = 1$，其他边界框 $\mathbb{1}_{ij}^{obj} = 0$。

经过以上优化后，最终的损失函数如下：

$$ \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right] $$

$$ + \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \left[ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] $$

$$ + \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} (C_i - \hat{C}_i)^2 $$

$$ + \lambda_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_i - \hat{C}_i)^2 $$

$$ + \sum_{i=0}^{S^2} \mathbb{1}_i^{obj} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2 $$

由上式可知，所有网格单元的所有边界框都会计算置信度损失，不同的是负责某目标的边界框计算置信度与 IOU 真实值的误差，其他边界框则计算置信度与 0 的误差，只有负责某对象的网格单元会计算分类损失，只有负责某对象的边界框会计算定位损失。

在来自 PASCAL VOC 2007 和 2012 的训练集和测试集上对网络进行了约 135 个周期的训练。在对 VOC 2012 进行测试时，训练集额外添加了 VOC 2007 的测试集。使用 64 的批量大小、0.9 的动量、0.0005 的衰减率。以 10−2 的学习率训练 75 个周期，然后以 10−3 的学习率训练 30 个周期，最后以 10−4 的学习率训练 30 个周期。

在第一个全连接层后引入 0.5 的 Dropout 层避免过拟合。对数据进行数据增强，引入最高幅度为原始图像 20% 的随机缩放和平移，以及在 HSV 颜色空间中最高调整幅度为 1.5 倍的曝光度和饱和度随机调整。

### 3.5. 推断

因为对一张图像仅需进行一次预测，YOLO 的运行速度极快。将图像划分为网格单元的设计使得模型能关注整张图像，提高对不同位置的检测能力。对于横跨多个网格单元的图像，采用 NMS 方法，仅保留负责该对象的边界框中置信度最高的一个，避免单个对象重复检测的问题。

### 3.6. 对比

* **Deformable parts models**

  YOLO 更快、更准确，所有工作均由一个神经网络完成，且特征提取和目标检测一起进行到端到训练，特征表示更适合检测任务。

* **R-CNN**

  YOLO 的空间约束可以减少同一对象的多次检测，边界框数量更少。R-CNN 则需要更复杂的流程和更慢的运行速度实现 YOLO 单一神经网络完成的工作。

* **Deep MultiBox**

  Deep MultiBox 无法执行通用对象检测，YOLO 则是一种通用检测器，能够同时学习检测多种不同的物体。

* **OverFeat**

  OverFeat 对定位性能进行优化，在进行预测时仅关注图像的一部分，需要大量的后处理以生成连贯的检测结果。

* **MultiGrasp**

  MultiGrasp 只需在图像上找到对象上的一个点以进行抓取，无需预测对象的大小、位置、边界、类别。

## 4. 实验

作者将 YOLO 和 Fast YOLO 与其他模型进行平均精度均值 mAP 的比较，且重点比较同样有实时性能的 DPM，结果显示，在 PASCAL 数据集上，Fast YOLO 是运行速度最快的目标检测模型，且 mAP 达到 52.7%，是此前最优记录的一倍以上，YOLO 在保持实时性能的同时 mAP 达到了 63.4%。

作者分析了 YOLO 在 VOC 2007 数据集上的错误，定位错误占其中一半以上，相比之下，此前表现最佳的 Fast R-CNN 出现了更多的背景错误，将背景误判为对象的概率接近 YOLO 的三倍。

作者组合 YOLO 和 Fast R-CNN，对 R-CNN 预测的每一个边界框，检查 YOLO 是否预测出类似边界框，并根据 YOLO 预测的概率以及两个边界框的重叠程度来提升该预测的置信度。结果显示，在 VOC 2007 测试集上，相比单独使用 Fast R-CNN，mAP 提升了 3.2%，达到 75.0%，提升幅度显著高于将多个 Fast R-CNN 进行组合的结果。但这种组合无法利用 YOLO 的速度优势。

在 VOC 2012 测试集上，YOLO 的 mAP 为 57.9%，低于当前最优水平，但接近使用 VGG-16 的原始 R-CNN。主要困难出现在对小型对象的识别上，在小型目标检测时，YOLO 得分低于 R-CNN 8%-10%，但在大型目标检测时，YOLO 得分更高。

在 VOC 训练集上训练后，在毕加索数据集和人物艺术数据集上进行测试。结果显示，YOLO 的平均精度下降幅度显著小于其他模型，具有较强的泛化能力。这是因为与 DPM 类似，YOLO 对大小、形状、位置和相互关系进行建模，而艺术品和自然图像在大小和形状方面很相似，有利于 YOLO 进行预测。

作者将 YOLO 连接至网络摄像头，对其实时性能进行验证。结果显示，YOLO 虽然逐张处理图像，但能够检测移动中的物体以及物体外观的变化。

## 5. 点评

### 5.1. 优势

* **结构简单**

  YOLO 仅由一个神经网络组成，且内部结构相对简单，相对于以往的目标检测模型训练速度快，便于任务迁移。

* **运行效率高**

  YOLO 对整张图象仅需一次前向传播即可完成推理，能同时保持极快的运行速度和较高的准确率。

* **全局建模**

  网格划分强制 YOLO 关注整张图象，有助于模型预测对象所处的不同位置，学习对象间相互关系。

* **通用性强**

  不同于以往的目标检测模型需要多个分类器参与，YOLO 可独立完成多目标检测任务，具有较强的通用性。

### 5.2. 不足

* **空间约束**

  YOLO 对每个网格单元中边界框数量做了严格限制，每个网格单元只能负责一种对象的检测，每个边界框只能负责一个对象的检测，这就使得 YOLO 在检测大量同种物体时出现了困难。

* **边界框预测**

  YOLO 从现有数据中学习边界框的预测，因此难以准确预测形状罕见对象的边界框。
  
* **分辨率低**
  
  经过多次下采样后特征的空间分辨率较低，因此对小目标和复杂形状目标的预测精度较低。

* **IOU 敏感性**


  IOU 未对框的大小作出区分，因此小边界框的 IOU 对定位误差更敏感。导致模型在小目标检测上的定位误差对最终性能影响更大，是 YOLO 主要的误差来源之一。
